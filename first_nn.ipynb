{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515697b-66b3-4e8e-9569-c26ccfeb165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic structure for neuron values + constants related to training & the activation f'n\n",
    "\n",
    "ALPHA = 0.1  # leaky ReLU slope parameter\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, val, grad=0.0):\n",
    "        self.val = val\n",
    "        self.grad = grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Value obj: \" + str(self.val)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Value(self.val + other.val, self.grad)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Value(self.val * other.val, self.grad)\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.val > other.val\n",
    "\n",
    "    def reLU(self):\n",
    "        return Value(max(0, self.val), self.grad)\n",
    "\n",
    "    # leaky ReLU. slope parameter chosen arbitrarily\n",
    "    def LreLU(self):\n",
    "        if (self.val < 0):\n",
    "            return Value(ALPHA * self.val, self.grad)\n",
    "        return Value(self.val, self.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8825c90-c1ec-4d2a-9982-36f76ad247b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Structure\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "# 784 pixels for input, 10 possible outputs (0, 1,...,9), arbitrary hidden layers\n",
    "MLP_LAYOUT = [784, 100, 100, 10]\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, n, nout):\n",
    "        #self.weights = [Value(random.uniform(-0.3,0.3)) for i in range(nout)]  # random uniform init\n",
    "        std_dv = (2 / n) ** 0.5\n",
    "        self.weights = [Value(random.gauss(0, std_dv)) for i in range(nout)]  # He Kaiming init\n",
    "        self.val = Value(0.0)\n",
    "        self.bias = Value(0.0)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, n, nout):\n",
    "        self.neurons = [Neuron(n, nout) for i in range(n)]       \n",
    "        \n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, MLP_LAYOUT):\n",
    "        self.layers = [Layer(size, next_size) \n",
    "                       for size, next_size in zip(MLP_LAYOUT, MLP_LAYOUT[1:] + [0])]\n",
    "\n",
    "\n",
    "    # currently O(n^3) Tâˆ†T\n",
    "    def forward(self, inpt):\n",
    "        # for every layer in mlp.layers[1:]\n",
    "        # for every neuron2 in layer2\n",
    "            # accum_val = neuron2bias\n",
    "            # for every neuron1 in layer1\n",
    "                # accum_val += neuronval * weight2\n",
    "            # neuron2val = sigmoid(accum_val)\n",
    "        \n",
    "        # initialize neuron values to 0 before beginning\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.val.val = 0.0\n",
    "    \n",
    "        for i in range(len(inpt)):  # loading first layer of mlp\n",
    "            self.layers[0].neurons[i].val.val = inpt[i]\n",
    "    \n",
    "        for layer1, layer2 in zip(self.layers[:-1], self.layers[1:]):\n",
    "            for neuron2, n in zip(layer2.neurons, range(len(layer2.neurons))):\n",
    "                neuron2.val += neuron2.bias;  # load with its bias\n",
    "                for neuron1 in layer1.neurons:  # load with val*weight of each prev neuron\n",
    "                    neuron2.val += neuron1.val * neuron1.weights[n]\n",
    "                neuron2.val = neuron2.val.LreLU()  # commpression/activation f'n   \n",
    "    \n",
    "    # returns the result of the last forward pass\n",
    "    def result(self):\n",
    "        last_layer = self.layers[-1]\n",
    "        x = 0\n",
    "            \n",
    "        for i in range(1, len(last_layer.neurons)):\n",
    "            if (last_layer.neurons[i].val > last_layer.neurons[x].val):\n",
    "                x = i\n",
    "        return x\n",
    "\n",
    "    # squared loss\n",
    "    def loss(self, desired):\n",
    "        last_layer = self.layers[-1]\n",
    "        loss = 0.0\n",
    "        for i in range(len(last_layer.neurons)):\n",
    "            loss += (last_layer.neurons[i].val.val - desired[i])**2\n",
    "        return loss\n",
    "\n",
    "    # cross-entropy loss\n",
    "    def lossCE(self, desired):\n",
    "        neurons = self.layers[-1].neurons\n",
    "        x = max([neurons[i].val.val for i in range(10)])\n",
    "        denom = sum([math.exp(neurons[i].val.val - x) for i in range(10)])\n",
    "        softmax = math.exp(neurons[desired.index(1)].val.val - x) / denom\n",
    "        return -math.log(softmax)\n",
    "\n",
    "    \n",
    "    def backward(self, desired_output):\n",
    "        # 1) zero out previous grads\n",
    "        # 2) initialize grads of final layer considering cost f'n def'n\n",
    "        # 3) for each layer in mlp_reverse except final layer\n",
    "            # for each neuron in layer\n",
    "                # for each weight from neuron\n",
    "                    # gradw = (grad neuron of prev layer)*(sigmoid deriv)*val_neuron\n",
    "                # if not in last layer...\n",
    "                    # gradn = sum((grad neuron of prev layer)*(sigmoid deriv)*w)\n",
    "                    # gradb = sum((grad neuron of prev layer)*(sigmoid deriv)*1)\n",
    "        \n",
    "        # 1)\n",
    "        self.zero_grads()\n",
    "    \n",
    "        # 2)\n",
    "        self.initgrads(desired_output)\n",
    "\n",
    "        # 3)\n",
    "        for layer2, layer1 in zip(reversed(self.layers[:-1]), reversed(self.layers[1:])):\n",
    "            for neuron2 in layer2.neurons:\n",
    "                # keep in mind; # of weights in any neuron of layer1 = # of neurons in layer2\n",
    "                for neuron1, n in zip(layer1.neurons, range(len(layer1.neurons))):                    \n",
    "                    if (neuron1.val.val < 0): x = ALPHA  # for leaky ReLU, this would be x = 1\n",
    "                    else: x = 1  # x represents d(neuron1.val)/d(neuron1.val before ReLU) = dsig/da\n",
    "              \n",
    "                    neuron2.weights[n].grad = neuron1.val.grad * neuron2.val.val * x  # dC/dw = dC/dsig * dsig/da * da/dw\n",
    "                    neuron2.val.grad += neuron1.val.grad * neuron2.weights[n].val * x  # dC/da = sum(dC/dw * dw/da)\n",
    "                    neuron2.bias.grad += neuron1.val.grad * x  # dC/db = dC/dsig * dsig/da * da/db (last term is 1)\n",
    "                # If we are in the last layer, no need to compute gradb or gradn\n",
    "                # as they will never be used. Just unecessary\n",
    "                # what we want: \"while NOT in the last layer, then compute gradb & gradn\"     \n",
    "\n",
    "\n",
    "    def backward_v2(self):\n",
    "        # copy-paste of part 3 of backward()\n",
    "        for layer2, layer1 in zip(reversed(self.layers[:-1]), reversed(self.layers[1:])):\n",
    "            for neuron2 in layer2.neurons:\n",
    "                # keep in mind; # of weights in any neuron of layer1 = # of neurons in layer2\n",
    "                for neuron1, n in zip(layer1.neurons, range(len(layer1.neurons))):                    \n",
    "                    x = 1  # x represents d(neuron1.val)/d(neuron1.val before ReLU) = dsig/da\n",
    "                    y = 1\n",
    "                    if (neuron1.val.val < 0): x = ALPHA  #LReLU\n",
    "                    if (neuron2.val.val < 0): y = ALPHA\n",
    "              \n",
    "                    neuron2.weights[n].grad = neuron1.val.grad * neuron2.val.val * x  # dC/dw = dC/dsig * dsig/da * da/dw\n",
    "                    neuron2.val.grad += neuron1.val.grad * neuron2.weights[n].val * x  # dC/da = sum(dC/dw * dw/da)\n",
    "                    neuron2.bias.grad += neuron1.val.grad * x * neuron2.weights[n].val * y\n",
    "    \n",
    "    \n",
    "    # updates mlp using the gradients of each value\n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                # update bias\n",
    "                neuron.bias.val -= LEARNING_RATE * neuron.bias.grad\n",
    "                # update weights\n",
    "                for weight in neuron.weights:\n",
    "                    weight.val -= LEARNING_RATE * weight.grad\n",
    "                    \n",
    "\n",
    "    # initializes first gradient layer based on softmax and categorical CE loss\n",
    "    def initgrads(self, desired_output):\n",
    "        val_gradient = 0.0\n",
    "        \n",
    "        neurons = self.layers[-1].neurons\n",
    "        x = max([neurons[j].val.val for j in range(10)])  # to avoid math overflow error, shifts exp values towards 0\n",
    "        for i in range(len(desired_output)):\n",
    "            denom = sum([math.exp(neurons[j].val.val - x) for j in range(10)])\n",
    "            if desired_output[i] == 0:  # for non-target logits, grad is softmax of this logit\n",
    "                val_gradient = math.exp(neurons[i].val.val - x) / denom\n",
    "            else:  # for target logit, grad is [softmax of this logit - 1]\n",
    "                val_gradient = (math.exp(neurons[i].val.val - x) / denom) - 1\n",
    "            neurons[i].val.grad += val_gradient\n",
    "            \n",
    "            if neurons[i].val.val < 0:\n",
    "                neurons[i].bias.grad += val_gradient * ALPHA\n",
    "            else: neurons[i].bias.grad += val_gradient\n",
    "\n",
    "\n",
    "    def divide_first_layer_grads(self):\n",
    "        for neuron in self.layers[-1].neurons:\n",
    "            neuron.val.grad /= BATCH_SIZE\n",
    "            neuron.bias.grad /= BATCH_SIZE\n",
    "    \n",
    "\n",
    "    def zero_grads(self):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.val.grad = 0.0\n",
    "                neuron.bias.grad = 0.0\n",
    "                for weight in neuron.weights:\n",
    "                    weight.grad = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e622a1-18a2-4140-be4d-470cd806c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data driver\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "class Data():\n",
    "\n",
    "    # i=1 for testing, i=0 (default) for training\n",
    "    def __init__(self, num_exercises, i=0):\n",
    "        self.inpt = []\n",
    "        self.outpt = []\n",
    "        if (i == 0):\n",
    "            self.load_data(train_x, train_y, num_exercises)\n",
    "        else: \n",
    "            self.load_data(test_x, test_y, num_exercises)\n",
    "    \n",
    "    def load_data(self, t_x, t_y, n):\n",
    "        t = []\n",
    "        d = []\n",
    "        for i in range(n):  # loads n examples\n",
    "            self.inpt.append(t_x[i].flatten())\n",
    "            self.outpt.append(t_y[i].flatten())\n",
    "        self.normalize_inpt()\n",
    "        self.normalize_outpt()\n",
    "        return (t, d)\n",
    "\n",
    "    # maps 0-255 pixel values to values b/t 0 and 1\n",
    "    def normalize_inpt(self):\n",
    "        for i in range(len(self.inpt)):\n",
    "            self.inpt[i] = self.inpt[i] / 255.0\n",
    "\n",
    "    # casts desired output into list with a 1 at the index equal to \n",
    "    # desired output (i.e. if desired output is 6, gives \n",
    "    # desired[i] = [0 0 0 0 0 0 1 0 0 0])\n",
    "    def normalize_outpt(self):\n",
    "        for i in range(len(self.outpt)):\n",
    "            x = self.outpt[i].item()\n",
    "            self.outpt[i] = []\n",
    "            for j in range(10):  # 10 possible digits, one index for each\n",
    "                if (x == j): self.outpt[i].append(1)\n",
    "                else: self.outpt[i].append(0)\n",
    "\n",
    "# loading training data\n",
    "train = Data(60000)\n",
    "print(len(train.inpt[0]), train.outpt[0])\n",
    "\n",
    "# loading testing data\n",
    "test = Data(10000, 1)\n",
    "print(len(test.inpt[0]), test.outpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34c7f2-fb21-4ab3-a8ea-64608bdc1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all ANN info to external csv for debugging and general analysis\n",
    "\n",
    "import csv\n",
    "EMPTY = \"          \"\n",
    "with open('0_all_biases.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['B0', 'B1', 'B2', 'B3', '', 'BG0', 'BG1', 'BG2', 'BG3'])\n",
    "    \n",
    "    for i in range(max([len(mlp.layers[i].neurons) for i in range(len(mlp.layers))])):  # 'for neuron in layer with most neurons'\n",
    "        row = [0.0 for j in range(2 * len(mlp.layers) + 1)]\n",
    "        for j in range(len(mlp.layers)):  # biases\n",
    "            try: row[j] = mlp.layers[j].neurons[i].bias.val\n",
    "            except: row[j] = EMPTY\n",
    "        row[len(mlp.layers)] = EMPTY  # space column for readability\n",
    "        for j in range(len(mlp.layers) + 1, 2*len(mlp.layers) + 1):  # bias gradients\n",
    "            try: row[j] = mlp.layers[j - len(mlp.layers) - 1].neurons[i].bias.grad\n",
    "            except: row[j] = EMPTY\n",
    "        writer.writerow(row)\n",
    "\n",
    "with open('0_all_neurons.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['V0', 'V1', 'V2', 'V3', '', 'VG0', 'VG1', 'VG2', 'VG3'])\n",
    "    \n",
    "    for i in range(max([len(mlp.layers[i].neurons) for i in range(len(mlp.layers))])):  # 'for neuron in layer with most neurons'\n",
    "        row = [0.0 for j in range(2 * len(mlp.layers) + 1)]\n",
    "        for j in range(len(mlp.layers)):  # values\n",
    "            try: row[j] = mlp.layers[j].neurons[i].val.val\n",
    "            except: row[j] = EMPTY\n",
    "        row[len(mlp.layers)] = EMPTY  # space column for readability\n",
    "        for j in range(len(mlp.layers) + 1, 2*len(mlp.layers) + 1):  # value gradients\n",
    "            try: row[j] = mlp.layers[j - len(mlp.layers) - 1].neurons[i].val.grad\n",
    "            except: row[j] = EMPTY\n",
    "        writer.writerow(row)\n",
    "\n",
    "with open('0_all_weights.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['W0', 'W1', 'W2', 'W3', '', 'WG0', 'WG1', 'WG2', 'WG3'])\n",
    "\n",
    "    for i in range(max([len(mlp.layers[x].neurons) for x in range(len(mlp.layers))])):  # 'for neuron in layer with most neurons'\n",
    "        row = [0.0 for j in range(2 * len(mlp.layers) + 1)]\n",
    "        for w in range(max([len(mlp.layers[x].neurons[0].weights) for x in range(len(mlp.layers))])):  # 'for weight in layer with most weights'\n",
    "            for j in range(len(mlp.layers)):  # weights\n",
    "                try: row[j] = mlp.layers[j].neurons[i].weights[w].val\n",
    "                except: row[j] = EMPTY\n",
    "            row[len(mlp.layers)] = EMPTY  # space column for readability\n",
    "            for j in range(len(mlp.layers) + 1, 2*len(mlp.layers) + 1):  # weight gradients\n",
    "                try: row[j] = mlp.layers[j - len(mlp.layers) - 1].neurons[i].weights[w].grad\n",
    "                except: row[j] = EMPTY\n",
    "            writer.writerow(row)\n",
    "        writer.writerow([EMPTY for i in range(2 * len(mlp.layers) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bab94-7a4f-48db-90c6-bb11eb090e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a new ANN instance\n",
    "mlp = MLP(MLP_LAYOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad652ce-f1d2-4eb2-bfb8-9f21b6b9a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting to one training example\n",
    "\n",
    "TRAINING_EXAMPLE = 19\n",
    "print(train.outpt[TRAINING_EXAMPLE])\n",
    "\n",
    "for i in range(1000):\n",
    "    mlp.zero_grads()\n",
    "    mlp.forward(train.inpt[TRAINING_EXAMPLE])\n",
    "    print(mlp.lossCE(train.outpt[TRAINING_EXAMPLE]))\n",
    "    mlp.initgrads(train.outpt[TRAINING_EXAMPLE])\n",
    "    mlp.backward_v2()\n",
    "    mlp.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffc0a8-db1a-4090-a935-45fc95cc1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "test = Data(10000, 1)\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i in range(10000):\n",
    "    total += 1\n",
    "    mlp.forward(test.inpt[i])\n",
    "    if (test.outpt[i].index(1) == mlp.result()): \n",
    "        correct += 1\n",
    "        print(i, \"            exp: \", test.outpt[i].index(1), \"act: \", mlp.result(), \"         \", correct / total, \"             !\")\n",
    "    else: print(i, \"            exp: \", test.outpt[i].index(1), \"act: \", mlp.result(), \"         \", correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebffe4c-ab56-4568-a020-8b6bbacafdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "counter = 0\n",
    "l = [0.0 for i in range(BATCH_SIZE)]\n",
    "for i in range(52499, 60000):\n",
    "    mlp.forward(train.inpt[i])\n",
    "    mlp.initgrads(train.outpt[i])\n",
    "    l[counter] = mlp.lossCE(train.outpt[i])\n",
    "    counter += 1\n",
    "    \n",
    "    if (counter == BATCH_SIZE):\n",
    "        av_loss = sum(l) / BATCH_SIZE\n",
    "        print(i, \". \", av_loss)\n",
    "        counter = 0\n",
    "        mlp.divide_first_layer_grads()\n",
    "        mlp.backward_v2()\n",
    "        mlp.update()\n",
    "        mlp.zero_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cea4f8-94ae-4198-b790-cccf3427385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training to overfit on a particular set of training examples\n",
    "\n",
    "for i in range(30):\n",
    "    for j in range(160):\n",
    "        mlp.forward(train.inpt[i])\n",
    "        mlp.initgrads(train.outpt[i])\n",
    "        l[counter] = mlp.lossCE(train.outpt[i])\n",
    "        counter += 1\n",
    "        \n",
    "        if (counter == BATCH_SIZE):\n",
    "            av_loss = sum(l) / BATCH_SIZE\n",
    "            print(av_loss)\n",
    "            counter = 0\n",
    "            mlp.divide_first_layer_grads()\n",
    "            mlp.backward_v2()\n",
    "            mlp.update()\n",
    "            mlp.zero_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a82bda-2885-4251-935b-66884ef30e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
